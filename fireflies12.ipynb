{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1ubNgbYwa2OihveFjpm4L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","\n","num_fireflies = 20  # Number of fireflies (candidate routing paths)\n","num_nodes = 10  # Number of nodes in the FANET\n","fireflies = np.random.randint(0, num_nodes, (num_fireflies, num_nodes))\n","brightness = np.zeros(num_fireflies)  # Fitness values\n","\n","# Initialize delay_matrix and pdr_matrix with random values for demonstration.\n","# Replace this with your actual delay and PDR data.\n","delay_matrix = np.random.rand(num_nodes, num_nodes)\n","pdr_matrix = np.random.rand(num_nodes, num_nodes)\n","\n","def fitness_function(path, delay_matrix, pdr_matrix):\n","    total_delay = sum(delay_matrix[path[i], path[i+1]] for i in range(len(path)-1))\n","    total_pdr = np.prod([pdr_matrix[path[i], path[i+1]] for i in range(len(path)-1)])\n","\n","    # We want to minimize delay and maximize PDR\n","    return 1 / (total_delay + 1) * total_pdr\n","\n","def move_firefly(firefly_i, firefly_j, beta=1, alpha=0.2):\n","    \"\"\"Move firefly_i towards firefly_j with some randomness.\"\"\"\n","    for i in range(len(firefly_i)):\n","        if np.random.rand() < beta:  # Attraction to brighter firefly\n","            firefly_i[i] = firefly_j[i]\n","        firefly_i[i] += alpha * np.random.randint(-1, 2)  # Random movement\n","    return firefly_i\n","\n","max_iterations = 50\n","\n","for _ in range(max_iterations):\n","    for i in range(num_fireflies):\n","        brightness[i] = fitness_function(fireflies[i], delay_matrix, pdr_matrix)\n","\n","    # Move fireflies\n","    for i in range(num_fireflies):\n","        for j in range(num_fireflies):\n","            if brightness[j] > brightness[i]:  # Move i towards j\n","                fireflies[i] = move_firefly(fireflies[i], fireflies[j])\n","\n","    best_firefly = fireflies[np.argmax(brightness)]  # Best routing path found\n","\n","print(\"Optimized Routing Path:\", best_firefly)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0VYBztlqhEM","executionInfo":{"status":"ok","timestamp":1740295175652,"user_tz":-330,"elapsed":714,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}},"outputId":"8bd8d020-93fa-48ce-e86d-1ed46838a4c2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimized Routing Path: [0 1 1 7 0 1 1 0 3 7]\n"]}]},{"cell_type":"code","source":["# FOA Optimized Paths as Initial DQN Experience\n","best_firefly_paths = [firefly for firefly in fireflies[:5]]  # Take top 5 paths\n","\n","# Assuming each node in the path can represent a state and the next node as an action:\n","initial_experience = []\n","for path in best_firefly_paths:\n","    for i in range(len(path) - 1):\n","        state = path[i]\n","        action = path[i + 1]  # Next node in the path as the action\n","        # You will need to define how to determine reward and next_state based on your specific DQN logic\n","        reward = 0  # Placeholder\n","        next_state = 0  # Placeholder\n","        initial_experience.append((state, action, reward, next_state))"],"metadata":{"id":"2BHs1myDrmr7","executionInfo":{"status":"ok","timestamp":1740295280622,"user_tz":-330,"elapsed":17,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define DQN Model\n","class DQN(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, action_dim)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","# Training DQN with FOA Experience\n","dqn_model = DQN(state_dim=num_nodes, action_dim=num_nodes)\n","optimizer = optim.Adam(dqn_model.parameters(), lr=0.001)\n","loss_fn = nn.MSELoss()\n","\n","for state, action, reward, next_state in initial_experience:\n","    optimizer.zero_grad()\n","    # Convert state to one-hot encoded tensor\n","    state_tensor = torch.zeros(num_nodes, dtype=torch.float32)  # Create a tensor of zeros with size num_nodes\n","    state_tensor[state] = 1  # Set the element corresponding to the state to 1\n","\n","    action_tensor = torch.tensor(action, dtype=torch.long)\n","\n","    q_values = dqn_model(state_tensor)\n","    q_value = q_values[action_tensor]\n","\n","    loss = loss_fn(q_value, torch.tensor(reward, dtype=torch.float32))\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"id":"CaW0ywtNsedt","executionInfo":{"status":"ok","timestamp":1740295503967,"user_tz":-330,"elapsed":168,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}}},"execution_count":9,"outputs":[]}]}