{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOY3jyTtmQwxvZaOcqLmSdc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxEzMGKzpqzX","executionInfo":{"status":"ok","timestamp":1740479440495,"user_tz":-330,"elapsed":113600,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}},"outputId":"7017b2f4-2d90-4267-ec5e-9aed4bf9ef41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Collecting stable-baselines3\n","  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n","Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.5.0\n"]}],"source":["pip install numpy gym torch stable-baselines3\n"]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from gym import spaces\n","from stable_baselines3 import PPO\n","# Import DummyVecEnv from the correct location\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","from collections import deque\n","import random\n","\n","# Custom FANET Environment\n","class FANETRoutingEnv(gym.Env):\n","    def __init__(self, num_uavs=5):\n","        super(FANETRoutingEnv, self).__init__()\n","\n","        # Environment parameters\n","        self.num_uavs = num_uavs\n","        self.max_steps = 100\n","        self.current_step = 0\n","\n","        # Action & State Space\n","        self.action_space = spaces.Discrete(self.num_uavs)  # Choose next UAV for routing\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(self.num_uavs, 3), dtype=np.float32)\n","\n","        # UAV State [Link Quality, Energy Level, Queue Size]\n","        self.state = np.random.rand(self.num_uavs, 3)\n","\n","    def reset(self):\n","        \"\"\" Reset the environment at the beginning of an episode \"\"\"\n","        self.current_step = 0\n","        self.state = np.random.rand(self.num_uavs, 3)  # Reinitialize UAV states\n","        # Return the state without flattening\n","        return self.state\n","\n","    def step(self, action):\n","        \"\"\" Execute a routing action (selecting a UAV) and return new state, reward, done flag \"\"\"\n","        self.current_step += 1\n","\n","        # Reward function: Encourage good link quality & energy efficiency\n","        link_quality = self.state[action][0]\n","        energy = self.state[action][1]\n","        queue_size = self.state[action][2]\n","\n","        reward = (link_quality * 10) - (queue_size * 2) - ((1 - energy) * 5)\n","\n","        # Update state: Simulate dynamic changes in FANET\n","        self.state = np.random.rand(self.num_uavs, 3)\n","        done = self.current_step >= self.max_steps\n","\n","        # Return the state without flattening\n","        return self.state, reward, done, {}\n","\n","    def render(self, mode='human'):\n","        pass"],"metadata":{"id":"Kp4b3zKxqkad","executionInfo":{"status":"ok","timestamp":1740479773657,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class DQN(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim * 3, 64)  # Modified input dimension to accommodate state features\n","        self.fc2 = nn.Linear(64, 64)\n","        self.fc3 = nn.Linear(64, output_dim)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten the state before passing to fc1\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","class DQNAgent:\n","    def __init__(self, input_dim, output_dim):\n","        self.model = DQN(input_dim, output_dim)\n","        self.target_model = DQN(input_dim, output_dim)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n","        self.memory = deque(maxlen=1000)\n","        self.gamma = 0.99\n","        self.epsilon = 1.0\n","        self.epsilon_decay = 0.995\n","        self.epsilon_min = 0.01\n","\n","    def select_action(self, state):\n","        \"\"\" Choose action based on epsilon-greedy policy \"\"\"\n","        if np.random.rand() < self.epsilon:\n","            return np.random.randint(0, self.model.fc3.out_features)\n","        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","        return torch.argmax(self.model(state)).item()\n","\n","    def train(self, batch_size=32):\n","        \"\"\" Train the DQN model using experience replay \"\"\"\n","        if len(self.memory) < batch_size:\n","            return\n","        batch = random.sample(self.memory, batch_size)\n","        states, actions, rewards, next_states, dones = zip(*batch)\n","\n","        states = torch.tensor(states, dtype=torch.float32)\n","        actions = torch.tensor(actions, dtype=torch.int64)\n","        rewards = torch.tensor(rewards, dtype=torch.float32)\n","        next_states = torch.tensor(next_states, dtype=torch.float32)\n","        dones = torch.tensor(dones, dtype=torch.float32)\n","\n","        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n","        next_q_values = self.target_model(next_states).max(1)[0]\n","        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n","\n","        loss = nn.MSELoss()(q_values, target_q_values)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def store_experience(self, state, action, reward, next_state, done):\n","        \"\"\" Store experience in replay buffer \"\"\"\n","        self.memory.append((state, action, reward, next_state, done))\n"],"metadata":{"id":"L4aCYGskpvyU","executionInfo":{"status":"ok","timestamp":1740479933933,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["!pip install shimmy>=2.0\n"],"metadata":{"id":"1Enu-4grqwUc","executionInfo":{"status":"ok","timestamp":1740479940817,"user_tz":-330,"elapsed":2639,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["env = DummyVecEnv([lambda: FANETRoutingEnv(num_uavs=5)])\n","ppo_model = PPO(\"MlpPolicy\", env, verbose=1)\n","\n","print(\"Training PPO Model...\")\n","ppo_model.learn(total_timesteps=5000)\n","print(\"PPO Training Completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1NNYJp2pzs0","executionInfo":{"status":"ok","timestamp":1740479964422,"user_tz":-330,"elapsed":18969,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}},"outputId":"f39b4cc5-df69-4f6c-eced-bde246918ddb"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Training PPO Model...\n","-----------------------------\n","| time/              |      |\n","|    fps             | 301  |\n","|    iterations      | 1    |\n","|    time_elapsed    | 6    |\n","|    total_timesteps | 2048 |\n","-----------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 281          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 4096         |\n","| train/                  |              |\n","|    approx_kl            | 0.0064895884 |\n","|    clip_fraction        | 0.0193       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.61        |\n","|    explained_variance   | 0.00236      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 102          |\n","|    n_updates            | 10           |\n","|    policy_gradient_loss | -0.013       |\n","|    value_loss           | 310          |\n","------------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 349          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 17           |\n","|    total_timesteps      | 6144         |\n","| train/                  |              |\n","|    approx_kl            | 0.0072671473 |\n","|    clip_fraction        | 0.015        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.59        |\n","|    explained_variance   | 0.00198      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 151          |\n","|    n_updates            | 20           |\n","|    policy_gradient_loss | -0.00775     |\n","|    value_loss           | 465          |\n","------------------------------------------\n","PPO Training Completed.\n"]}]},{"cell_type":"code","source":["# Initialize Environment & DQN Agent\n","env = FANETRoutingEnv(num_uavs=5)\n","dqn_agent = DQNAgent(input_dim=env.observation_space.shape[0], output_dim=env.action_space.n)\n","\n","num_episodes = 100\n","\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        action = dqn_agent.select_action(state)  # DQN selects best routing link\n","        next_state, reward, done, _ = env.step(action)\n","\n","        dqn_agent.store_experience(state, action, reward, next_state, done)\n","        dqn_agent.train()\n","        state = next_state\n","        total_reward += reward\n","\n","    print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n","\n","# PPO Deployment\n","print(\"Deploying PPO for UAV Path Optimization...\")\n","ppo_obs = env.reset()\n","for _ in range(10):\n","    action, _ = ppo_model.predict(ppo_obs)\n","    ppo_obs, _, _, _ = env.step(action)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnQjP-mRp34b","executionInfo":{"status":"ok","timestamp":1740480011999,"user_tz":-330,"elapsed":35100,"user":{"displayName":"Anita Mhatre","userId":"12925641090053780434"}},"outputId":"1127584c-3d6c-4194-834f-026e10bcf5bc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1, Total Reward: 101.09856717679705\n","Episode 2, Total Reward: 185.96752300012176\n","Episode 3, Total Reward: 131.7517013427119\n","Episode 4, Total Reward: 154.69471538676382\n","Episode 5, Total Reward: 113.30963082406109\n","Episode 6, Total Reward: 189.04222462108527\n","Episode 7, Total Reward: 99.84993036926137\n","Episode 8, Total Reward: 134.12978487585576\n","Episode 9, Total Reward: 182.90377795875116\n","Episode 10, Total Reward: 159.6269745114348\n","Episode 11, Total Reward: 131.2391268291912\n","Episode 12, Total Reward: 167.7906883083219\n","Episode 13, Total Reward: 160.53533137801497\n","Episode 14, Total Reward: 178.46345729285454\n","Episode 15, Total Reward: 174.60055440529612\n","Episode 16, Total Reward: 176.84971725964974\n","Episode 17, Total Reward: 153.28844253250435\n","Episode 18, Total Reward: 181.1483172303795\n","Episode 19, Total Reward: 142.71605183653773\n","Episode 20, Total Reward: 129.61557026660157\n","Episode 21, Total Reward: 150.1934992662397\n","Episode 22, Total Reward: 156.92635000745275\n","Episode 23, Total Reward: 133.9745135808583\n","Episode 24, Total Reward: 182.08721601037146\n","Episode 25, Total Reward: 198.34001206456813\n","Episode 26, Total Reward: 147.0977262543248\n","Episode 27, Total Reward: 78.17924157964661\n","Episode 28, Total Reward: 129.67025738866732\n","Episode 29, Total Reward: 165.73185817749842\n","Episode 30, Total Reward: 168.35508132400514\n","Episode 31, Total Reward: 166.2315069181456\n","Episode 32, Total Reward: 170.45730676809765\n","Episode 33, Total Reward: 174.48720403908987\n","Episode 34, Total Reward: 177.2015201449644\n","Episode 35, Total Reward: 109.20389630588777\n","Episode 36, Total Reward: 165.68101077533595\n","Episode 37, Total Reward: 192.37847119599655\n","Episode 38, Total Reward: 178.21880195945462\n","Episode 39, Total Reward: 149.86877369198533\n","Episode 40, Total Reward: 137.26916728107724\n","Episode 41, Total Reward: 108.1398628676726\n","Episode 42, Total Reward: 175.66049773129768\n","Episode 43, Total Reward: 128.88205377914508\n","Episode 44, Total Reward: 134.36058494844985\n","Episode 45, Total Reward: 163.82333985547635\n","Episode 46, Total Reward: 77.12934835284376\n","Episode 47, Total Reward: 157.76607743758282\n","Episode 48, Total Reward: 158.91592538412849\n","Episode 49, Total Reward: 171.51975511486086\n","Episode 50, Total Reward: 165.19155374133123\n","Episode 51, Total Reward: 113.2378621153944\n","Episode 52, Total Reward: 121.89490050726809\n","Episode 53, Total Reward: 124.13526853150339\n","Episode 54, Total Reward: 188.69315741109054\n","Episode 55, Total Reward: 203.03745513861705\n","Episode 56, Total Reward: 103.86328015224144\n","Episode 57, Total Reward: 190.87335181454787\n","Episode 58, Total Reward: 112.65671742708129\n","Episode 59, Total Reward: 167.65538865004132\n","Episode 60, Total Reward: 93.5012140239128\n","Episode 61, Total Reward: 176.1856524964812\n","Episode 62, Total Reward: 160.54176536655567\n","Episode 63, Total Reward: 155.98077736927448\n","Episode 64, Total Reward: 134.19278190558597\n","Episode 65, Total Reward: 138.38029325499792\n","Episode 66, Total Reward: 138.63231900418086\n","Episode 67, Total Reward: 165.4078790139093\n","Episode 68, Total Reward: 145.8721557622635\n","Episode 69, Total Reward: 212.61279672987425\n","Episode 70, Total Reward: 161.1019653885224\n","Episode 71, Total Reward: 211.79333780055939\n","Episode 72, Total Reward: 204.9085280331818\n","Episode 73, Total Reward: 139.88161274744778\n","Episode 74, Total Reward: 133.5941907841945\n","Episode 75, Total Reward: 72.94437788276565\n","Episode 76, Total Reward: 132.8273123689044\n","Episode 77, Total Reward: 168.3818883471385\n","Episode 78, Total Reward: 151.9848977126845\n","Episode 79, Total Reward: 174.06916225183062\n","Episode 80, Total Reward: 156.54217864903595\n","Episode 81, Total Reward: 143.12829914499127\n","Episode 82, Total Reward: 165.82604311963738\n","Episode 83, Total Reward: 91.32389973704394\n","Episode 84, Total Reward: 200.74637456447968\n","Episode 85, Total Reward: 127.99538011046454\n","Episode 86, Total Reward: 184.49739853304774\n","Episode 87, Total Reward: 186.73578809534655\n","Episode 88, Total Reward: 181.97208992915077\n","Episode 89, Total Reward: 123.6089220403222\n","Episode 90, Total Reward: 97.7163806195508\n","Episode 91, Total Reward: 187.51244347886188\n","Episode 92, Total Reward: 131.0246348543843\n","Episode 93, Total Reward: 88.72704795335414\n","Episode 94, Total Reward: 182.95718925097776\n","Episode 95, Total Reward: 124.68040292567436\n","Episode 96, Total Reward: 142.74665289479623\n","Episode 97, Total Reward: 161.23819328243096\n","Episode 98, Total Reward: 192.40582719663405\n","Episode 99, Total Reward: 143.22286234155897\n","Episode 100, Total Reward: 173.91754577607134\n","Deploying PPO for UAV Path Optimization...\n"]}]}]}