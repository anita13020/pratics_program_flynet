{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxuiN4Gm4ZbsxwQwt19Hcc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CrMijkEZwUuY"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import lightgbm as lgb\n","import gym\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Step 1: Generate Synthetic FANET Dataset\n","np.random.seed(42)\n","n_samples = 5000\n","data = pd.DataFrame({\n","    'node_speed': np.random.uniform(0, 20, n_samples),  # Speed in m/s\n","    'link_quality': np.random.uniform(0, 1, n_samples),  # 0 (poor) to 1 (excellent)\n","    'congestion_level': np.random.uniform(0, 1, n_samples),\n","    'energy': np.random.uniform(10, 100, n_samples),  # Battery level in %\n","    'hop_count': np.random.randint(1, 10, n_samples),\n","    'best_next_hop': np.random.randint(0, 5, n_samples)  # 5 possible next-hop options\n","})\n","\n","data.to_csv('fanet_routing_data.csv', index=False)\n","\n","# Step 2: Feature Engineering\n","features = ['node_speed', 'link_quality', 'congestion_level', 'energy', 'hop_count']\n","target = 'best_next_hop'\n","\n","X = data[features]\n","y = data[target]\n","\n","# Normalize Features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Split Data\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Step 3: Train GBDT Model\n","gbdt_model = lgb.LGBMClassifier()\n","gbdt_model.fit(X_train, y_train)\n","\n","def gbdt_predict(state):\n","    state_scaled = scaler.transform([state])\n","    return gbdt_model.predict(state_scaled)[0]\n","\n","# Step 4: Reinforcement Learning with DQN\n","class FANETEnv(gym.Env):\n","    def __init__(self):\n","        self.state_space = gym.spaces.Box(low=0, high=1, shape=(len(features),), dtype=np.float32)\n","        self.action_space = gym.spaces.Discrete(5)  # Assume 5 routing choices\n","        self.state = np.random.rand(len(features))\n","\n","    def step(self, action):\n","        reward = np.random.rand()  # Simulated reward\n","        self.state = np.random.rand(len(features))\n","        return self.state, reward, False, {}\n","\n","    def reset(self):\n","        self.state = np.random.rand(len(features))\n","        return self.state\n","\n","env = FANETEnv()\n","\n","# Define RL Model (Deep Q-Network)\n","n_inputs = len(features)\n","n_outputs = 5  # Number of routing options\n","\n","model = tf.keras.Sequential([\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(n_outputs, activation='linear')\n","])\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n","\n","# Hybrid Routing Decision Function\n","def hybrid_routing(state):\n","    gbdt_action = gbdt_predict(state)\n","    if np.random.rand() < 0.8:\n","        return gbdt_action  # Use GBDT 80% of the time\n","    else:\n","        q_values = model.predict(np.array([state]))\n","        return np.argmax(q_values)\n","\n","# Example Decision\n","state = np.random.rand(len(features))\n","chosen_action = hybrid_routing(state)\n","print(\"Selected Routing Action:\", chosen_action)\n","\n","\n"]}]}